{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7Jn4FaNWNSSD5jXwgcTV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wmalevich/ds_course/blob/main/hw_4(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Комбинированные (Hybrid) Алгоритмы.\n",
        "\"\"\"\n",
        "Мы создаём простую среду с определенным количеством состояний и действий.\n",
        "Метод step в классе SimpleEnv определяет, как изменяется состояние и какая награда выдается в зависимости от действия агента.\n",
        "Метод reset сбрасывает среду в начальное состояние.\n",
        "Функция hybrid_policy определяет гибридную политику: если для текущего состояния в Q-таблице есть положительные значения,\n",
        "выбирается действие с максимальным значением; в противном случае действие выбирается случайно.\n",
        "Функция train обучает агента, обновляя Q-таблицу на основе взаимодействия с средой.\n",
        "После обучения мы выводим Q-таблицу, которая показывает, какое действие является наилучшим для каждого состояния с точки зрения ожидаемой награды.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class SimpleEnv:\n",
        "    def __init__(self):\n",
        "        # Определяем размер пространства состояний и действий\n",
        "        self.state_space_size = 4  # Например, 4 различных состояния\n",
        "        self.action_space_size = 2  # Например, 2 действия (0 и 1)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        # Метод, определяющий логику изменения состояния и назначения награды\n",
        "        # Пример: состояние циклически увеличивается и обнуляется\n",
        "        next_state = (state + 1) % self.state_space_size\n",
        "        # Награда зависит от выбранного действия\n",
        "        reward = 1 if action == 0 else -1\n",
        "        # Проверяем, завершился ли эпизод (для упрощения считаем, что после каждого действия эпизод завершается)\n",
        "        done = next_state == 0\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        # Возвращает начальное состояние среды\n",
        "        return 0\n",
        "\n",
        "def hybrid_policy(q_table, state):\n",
        "    # Гибридная политика: используем Q-таблицу, если есть положительные значения, иначе случайный выбор\n",
        "    if np.max(q_table[state]) > 0:\n",
        "        return np.argmax(q_table[state])\n",
        "    else:\n",
        "        return random.choice([0, 1])\n",
        "\n",
        "def train(env, episodes=1000, learning_rate=0.8, gamma=0.95):\n",
        "    # Инициализация Q-таблицы нулями\n",
        "    q_table = np.zeros((env.state_space_size, env.action_space_size))\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = hybrid_policy(q_table, state)  # Выбор действия согласно гибридной политике\n",
        "            next_state, reward, done = env.step(state, action)\n",
        "\n",
        "            # Обновление Q-таблицы\n",
        "            q_table[state, action] = (1 - learning_rate) * q_table[state, action] + \\\n",
        "                                     learning_rate * (reward + gamma * np.max(q_table[next_state]))\n",
        "            state = next_state\n",
        "\n",
        "    return q_table\n",
        "\n",
        "# Создаем экземпляр среды и обучаем агента\n",
        "env = SimpleEnv()\n",
        "q_table = train(env)\n",
        "\n",
        "# Выводим обученную Q-таблицу\n",
        "print(\"Обученная Q-таблица:\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L8VSfa5Dhgp",
        "outputId": "edb516cd-fea6-4a29-f73c-acabae75eb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обученная Q-таблица:\n",
            "[[ 9.75706729  0.        ]\n",
            " [ 0.          9.21796556]\n",
            " [10.75575323 -0.8       ]\n",
            " [10.26921392  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class QLearningWrapper(BaseEstimator):\n",
        "    def __init__(self, env, episodes=1000, learning_rate=0.8, gamma=0.95):\n",
        "        self.env = env\n",
        "        self.episodes = episodes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        q_table = np.zeros((self.env.state_space_size, self.env.action_space_size))\n",
        "\n",
        "        for episode in range(self.episodes):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = hybrid_policy(q_table, state)  # Выбор действия согласно гибридной политике\n",
        "                next_state, reward, done = self.env.step(state, action)\n",
        "\n",
        "                # Обновление Q-таблицы\n",
        "                q_table[state, action] = (1 - self.learning_rate) * q_table[state, action] + \\\n",
        "                                         self.learning_rate * (reward + self.gamma * np.max(q_table[next_state]))\n",
        "                state = next_state\n",
        "\n",
        "        self.q_table_ = q_table\n",
        "        return self\n",
        "\n",
        "    def score(self, X=None, y=None):\n",
        "        return 0\n",
        "\n",
        "# Создаем экземпляр среды\n",
        "env = SimpleEnv()\n",
        "\n",
        "# Определяем диапазоны для подбора гиперпараметров\n",
        "param_dist = {\n",
        "    'episodes': [1000, 2000],\n",
        "    'learning_rate': np.arange(0.1, 1.0, 0.1),\n",
        "    'gamma': np.arange(0.1, 1.0, 0.1)\n",
        "}\n",
        "\n",
        "# Создаем пустые массивы для данных X и y\n",
        "X_dummy = np.empty((10, env.state_space_size))\n",
        "y_dummy = np.empty(10)\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=QLearningWrapper(env), param_distributions=param_dist, cv=3, n_iter=10)\n",
        "\n",
        "random_search.fit(X=X_dummy, y=y_dummy)\n",
        "\n",
        "best_params = random_search.best_params_\n",
        "print(\"Лучшие параметры:\", best_params)\n",
        "\n",
        "# Получаем обученную модель с лучшими параметрами\n",
        "best_q_table = random_search.best_estimator_\n",
        "\n",
        "# Выводим обученную Q-таблицу\n",
        "print(\"Обученная Q-таблица с лучшими параметрами:\")\n",
        "print(best_q_table.q_table_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taxKbb9aGTKs",
        "outputId": "d9aab9f2-ce39-4e6a-c5ff-335e88431d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры: {'learning_rate': 0.9, 'gamma': 0.8, 'episodes': 1000}\n",
            "Обученная Q-таблица с лучшими параметрами:\n",
            "[[ 2.83197832  0.        ]\n",
            " [ 2.2899729  -0.9       ]\n",
            " [ 0.          1.61246612]\n",
            " [ 3.26558266 -0.252     ]]\n"
          ]
        }
      ]
    }
  ]
}